Thanks for the clarification. I’ll now conduct a comprehensive research task covering:

* A literature and codebase survey of SOTA reaction prediction models and relevant MoE approaches
* Exploration of publicly available reaction datasets with condition or 3D annotations
* Design recommendations for a heterogeneous MoE system with modular experts and gating
* A detailed implementation and test plan
* A unified research report including a modular repo structure and architecture mind-map

I’ll get back to you once the report is ready.


## 1. **State-of-the-Art Reaction Prediction Models**

Modern reaction outcome predictors vary in how they represent chemical data and the neural architectures they employ. Below we summarize key models and their innovations:

* **Graph2SMILES (Tu & Coley, 2021):** A graph-to-sequence Transformer that encodes reactants as molecular graphs and decodes the product as a SMILES string. Its encoder uses a directed message-passing neural network (D-MPNN) with attention and **graph-aware positional embeddings** based on inter-atom shortest paths. This yields a permutation-invariant representation (insensitive to atom or reagent ordering). A standard Transformer decoder then generates the product SMILES. Graph2SMILES achieved **90.3% top-1 accuracy** on the USPTO-480k dataset (mixed format) and **78.1%** on the USPTO-STEREO set. These results are \~1.7–1.9% higher than the Molecular Transformer baseline, indicating improved performance especially for stereochemistry. The model also improved the diversity of beam search outputs and robustness to input ordering. *Innovation:* Permutation-invariant graph encoding removed the need for data augmentation (randomizing SMILES) and improved capturing of reactant connectivity.

* **Molecular Transformer (Schwaller et al., 2019):** A seminal sequence-to-sequence model treating reaction SMILES as a “language translation” task. It uses the original Transformer architecture with **4 encoder and 4 decoder layers** (≈12 million parameters) and positional encodings. Reactants and reagents are input as tokenized SMILES, either separated by special `>` tokens or fully concatenated (mixed). Data augmentation via SMILES randomization was used during training. This model was the first to break the 90% accuracy barrier on USPTO-480k, achieving **90.4% top-1** when reactants/reagents were separated and **88.6%** in the mixed format. It significantly outperformed earlier template-based methods by over 10 percentage points. *Innovation:* Demonstrated that a pure data-driven Transformer can learn chemical reaction mappings, introducing a tailored SMILES tokenizer and showing Transformers can capture chemistry without hand-coded rules. It also incorporated an uncertainty estimation mechanism (the model could assess confidence in its predictions).

* **Chemformer (Irwin et al., 2022):** A pre-trained Transformer (based on the BART encoder–decoder) fine-tuned for reactions. Chemformer was pre-trained on **100 million SMILES** from ZINC-15 via self-supervised tasks (token masking and SMILES denoising). Two model sizes were used (≈45M and 230M parameters). Fine-tuning on USPTO-480k led to state-of-the-art results: the base model achieved \~**90.6–90.9%** top-1 accuracy on USPTO (mixed input), improving to \~**92–92.5%** when reactants were separated. The larger model slightly exceeded these (up to \~92.8% top-1). For retrosynthesis on USPTO-50k, Chemformer reported \~54% top-1 (vs \~52–53% for non-pretrained models). *Innovation:* Demonstrated the benefit of **pre-training on unlabeled chemical data**. The model learned a “chemical language” representation that improved sample efficiency and accuracy on reaction prediction. By combining generative pre-training and fine-tuning, Chemformer showed robust performance across forward prediction, retrosynthesis, and even molecular property prediction tasks.

* **3D Infomax (Stärk et al., 2022):** A strategy to inject 3D molecular knowledge into 2D graph models via self-supervised pre-training. A GNN encoder is trained to **maximize mutual information** between a molecule’s 2D graph representation and its 3D geometry representation. Concretely, the authors used \~**1.1 million molecules** with known conformers (from datasets **QM9**, **GEOM-Drugs**, **QMugs**) to create positive pairs of 2D graphs and 3D coordinates. The model learns a latent 3D vector from distances/angles which must align with the 2D GNN’s output for the same molecule. After this pre-training, the 2D GNN alone is fine-tuned on tasks without needing 3D input. This yielded sizable gains: e.g. a **22% reduction in MAE** on QM9 quantum property predictions versus no pre-training. It also improved molecular property classification performance across MoleculeNet benchmarks, often outperforming other self-supervised methods. *Innovation:* **Implicit 3D knowledge** – 3DInfomax endows a standard 2D model with an intuition for 3D structure, without requiring 3D coordinates at inference. This is valuable for reaction tasks, suggesting that a model can be pre-trained on 3D geometry and then applied to reaction data (where reactant 3D conformers might not be available in real time). It also introduced multi-conformer training (aggregating info from multiple conformers per molecule) to enrich the learned representation.

* **ChemTransformer (Internal, 2023):** A flexible Transformer-based **template** developed in-house for reaction modeling. It introduced an “**event-based**” SMILES representation enriched with auxiliary tokens for reactant roles and conditions. In the input sequence, each molecule is preceded by a token indicating if it’s a `[REACTANT]`, `[CATALYST]`, `[SOLVENT]`, etc., and optional numeric tokens denote conditions like temperature or amount. This helps the model understand the context of each component. ChemTransformer uses learned positional embeddings that reset at the start of each reactant/reagent, implemented via a custom positional indexing scheme. The architecture is a standard 6-layer Transformer encoder–decoder with 8 heads and 512-dimensional embeddings. Although no official benchmark results were published (it’s a platform for experimentation), this design emphasizes **modularity and extensibility**. *Innovation:* The ability to ingest structured reaction information (roles, conditions) and **learn positional encodings at the compound level**. This makes it easy to integrate additional data (like catalysts or temperatures) and compare tokenization strategies (SMILES vs. SELFIES) in a unified framework. ChemTransformer will serve as a foundation to build our mixture-of-experts system, given its configurable encoding of reactions.

**MoE in Chemical Reaction Prediction – Recent Work:** Most reaction models so far have been single-expert. However, a 2024 study by Guo *et al.* introduced a **Sequential Mixture-of-Experts** approach to better capture *rare* reaction patterns. Their motivation was that conventional models tend to predict the most common product outcome, missing less-frequent but chemically plausible outcomes. To address this, they trained multiple expert models, each specializing in a different **electron redistribution pattern** (i.e. a different reaction mechanism or electron flow). Instead of a single gating network, they used a **boosting-like sequence** of experts: after an initial warm-up training, they iteratively re-trained experts on subsets of the data that were poorly handled by the previous ones. This process (“Boosting Training”) encourages experts to become diverse, focusing on different reaction subspaces. During inference, they employed a **similarity-based gating**: given a new reactant set, they compute a fingerprint and assign the reaction to the most similar expert (based on training set patterns). Additionally, they apply **dropout at inference** time within each expert to stochastically vary the predictions. By combining outputs from all experts and ranking them, the method generates a list of plausible products that covers both the typical and the rarer outcomes. On USPTO-480k (USPTO-MIT), this sequential-MoE approach achieved higher top-N accuracies than single-model baselines – for example, their full approach (no CVAE prior + boosting + dropout) reached **95.6% top-5 accuracy**, vs. \~94% for a single-model counterpart. *Key distinctions from classical MoE:* (1) Gating is done via a retrieval-based scheme (pattern similarity) rather than a learned parametric function of the input; (2) Experts are trained sequentially (boosted) rather than simultaneously, which is more like an ensemble – albeit one where each expert specializes due to targeted training; (3) Diversity is encouraged through training data partitioning and inference dropout, not via a continuous probabilistic gating function. This is a promising approach for reactions because it explicitly acknowledges **multi-modal output distributions** (multiple possible products) rather than collapsing to one “average” prediction.

**MoE Architectures in Other Domains:** We can draw inspiration from Mixture-of-Experts research in NLP and vision for designing our heterogeneous MoE:

* **Switch Transformer (Fedus et al., 2021):** Google’s Switch-C introduced an MoE layer in Transformers where a simple gating network routes each input **token** to a single expert out of many. This **top-1 gating** drastically reduces computation (only one expert’s feed-forward network is active per token) while allowing model capacity to scale (e.g. thousands of experts). Switch uses a lightweight router (essentially a softmax over expert-key scores) and an auxiliary loss to keep the load balanced across experts. Notably, experts in Switch share the same architecture and input space – they are all copies of the Transformer feed-forward block – so this is a *homogeneous* MoE. Yet, the gating mechanism and training tricks (load balancing, random routing noise) from Switch are relevant: they prevent any single expert from dominating and ensure each expert specializes on a subset of the data. We may adapt similar ideas (e.g. a load-balancing loss) when training our experts to avoid “collapse” where one expert learns most reactions and others are under-utilized.

* **MMoE (Multi-gate MoE, Ma et al., 2018):** A technique from multi-task learning where **multiple gating networks** share a common pool of experts. Each task (or output head) has its own gate that learns a weighting over experts, allowing it to preferentially use experts that are most helpful for that task. The experts themselves are shared across all tasks. This approach was used for recommendation systems (e.g. predicting multiple user engagement metrics simultaneously). The concept applicable to us is that **different “contexts” can activate different combinations of experts**. In our case, instead of tasks we have different reaction types or input modalities. MMoE suggests that a single unified gating may be suboptimal if the data is heterogeneous – instead, gating could consider multiple factors (for instance, a gate for reactions needing 3D reasoning vs. a gate for reactions predictable by 2D patterns). While we likely will use one gating network (since we have a single task: forward prediction), the idea of multi-gate could inform a hierarchical gating design (first choose modality, then maybe a finer pattern-based expert).

* **MoL-MoE (Multi-**o**bjective **l**earning MoE, IBM Research 2025):** A recent approach integrating **multiple input modalities** for molecular property prediction. In an ACS Spring 2025 abstract, Shirasuna *et al.* describe a system with **12 experts divided evenly among 3 input representations**: SMILES sequences, SELFIES sequences, and molecular graphs. A gating network fuses the latent features from these three views and routes each input to a subset of experts from all three categories. By leveraging complementary representations, their MoE achieved superior accuracy across various MoleculeNet property datasets. This is directly analogous to our goal: **heterogeneous experts handling different input types**. It shows that gating can successfully operate over experts with distinct architectures. One design from MoL-MoE is to partition experts by modality (so each modality has a few dedicated experts). They report using \$k=4\$ or \$k=6\$ experts per inference, meaning the gate can activate multiple experts and combine their outputs. This soft gating (top-\$k\$ routing) might boost performance by ensembling experts, though it increases computation. We will consider both hard gating (pick one expert per reaction) and soft gating (mix a few experts) in our design, weighing accuracy vs. efficiency.

* **Other MoE Examples:** Many large language models (e.g. GLaM, 2022) and vision models (e.g. Vision MoE in Google’s ViT-MoE) use experts to scale model size. Techniques like **Expert Layers vs. Expert Tokens** have been explored – e.g. some architectures route entire *layers* of a network to different experts or use expert networks at different depths for different aspects of the input. There are also approaches to encourage expert specialization (beyond load-balancing, some use entropy regularization on gating outputs or expert dropout during training to force diversity). These lessons all converge on a few principles for MoEs: (*a*) **Sparsity** – only a subset of experts active per input to save compute; (*b*) **Diversity** – experts need incentive to specialize (via gating structure or training loss); (*c*) **Modularity** – experts can have different capacities or even architectures, but the system should be modular enough that one can plug in a new expert without retraining the others from scratch. We will apply these principles in designing our system.

## 2. **Datasets and Annotations for Multi-Expert Training**

Our primary training data will be the **USPTO-480k** patent reaction set (also called USPTO-MIT) of 479k atom-mapped reactions. This is the standard benchmark for one-step reaction prediction, but it contains only reactant and major product SMILES (no detailed conditions or 3D information). To enrich our experts, we should leverage additional datasets or augmentations:

* **Reaction Condition Data:** *Catalysts, solvents, temperatures, yields.* While USPTO patents contain text describing conditions, this information isn’t structured in the 480k dataset. However, recent open resources aim to provide it. For example, the Open Reaction Database (ORD, 2022) is a public repository of organic reactions including conditions, experimental details, and sometimes yields. Similarly, researchers have created a **USPTO-Condition** subset by text-mining the original USPTO corpus for reaction conditions. One paper curated “USPTO-Condition” with standardized reagent and condition annotations from the patents – this could supply us with labels like solvents used, temperature, or time for a subset of USPTO reactions. For yields, the USPTO patent text often includes yield percentages, but they can be biased (e.g. high yields reported more often for certain scales). A study by IBM noted that yields extracted from USPTO have a bimodal distribution depending on reaction scale, limiting their direct use for ML models. Instead, high-quality yield data come from *high-throughput experimental (HTE)* studies. For instance, the Buchwald-Hartwig amination and Suzuki coupling datasets (Ahneman *et al.*, Science 2018 and Gao *et al.*, 2018) contain hundreds or thousands of measured yields under various catalyst/base conditions. These HTE datasets, though smaller (\~10³ examples each), are useful for an expert focusing on **reaction yield prediction** or condition optimization tasks. In our context, we might include them if we create an expert that predicts not just the product structure but some quantitative aspect (or at least uses conditions as input features). As a starting point, we will incorporate **reaction role labels and basic conditions** into the input for a “Condition-aware Expert” using the same format as ChemTransformer (role tokens + optional numeric tokens) on the USPTO-480k data. This doesn’t require a new dataset, just parsing the existing one with additional metadata (we have a mapping of common reagents to categories, etc.). For more diverse condition data, we can augment from ORD or USPTO-Condition if available, ensuring to align their format with our model. Since condition annotations might not be available for all USPTO reactions, we could use a multi-task learning approach: e.g. train the condition expert to predict products *and* conditions (as separate outputs) for those reactions where conditions are known.

* **3D Structure Data:** No large reaction dataset includes 3D conformers for reactants by default. However, we can leverage molecular datasets like QM9, GEOM-Drugs, and GEOM-QM9 (QMugs) used in 3DInfomax to *pre-train* a **3D-informed expert**. Concretely, we can take the GNN encoder from 3DInfomax (or a similar model) pre-trained on those datasets (which contain equilibrium 3D coordinates for individual molecules). Then, for each reactant in a training reaction, we generate a 3D conformer (using RDKit or another conformer generator) and feed it through this pre-trained encoder to get an enriched reactant embedding. These embeddings can be input to the expert’s model (for instance, concatenated with a 2D graph embedding). Importantly, because 3DInfomax’s encoder does *not* require 3D input at fine-tuning time (it produces a 2D graph encoder imbued with 3D knowledge), we could simply use the pre-trained weights on our reaction dataset without explicitly generating conformers for every reactant. Nonetheless, if we want to push a dedicated 3D expert, we might compile a small dataset of reactions with known or easily computed 3D information: for example, reactions from the **USPTO-STEREO** set (which has stereochemistry) could be augmented by generating 3D coordinates for stereochemically-defined reactants to train the model to preserve stereochemical outcomes. Another possibility is to simulate reaction complexes or transition state analogs (if data from reaction kinetics studies or quantum chemistry calculations were available for some reactions, they could provide 3D features). For now, the pragmatic approach is: use **pre-trained 3D GNN features** for reactants. This means our 3D expert can take as input either (a) the raw SMILES plus a learned 3D embedding vector per molecule, or (b) a 3D coordinate set for each reactant (from a force-field optimization) processed by a frozen GNN. We will assess feasibility – generating one 3D conformer per reactant is not too expensive and could be done on-the-fly during training as a form of data augmentation for the 3D expert. To align with 3DInfomax, we’ll ensure to use the same atom feature set (element type, formal charge, etc.) and maybe augment with distance features between reactants if relevant (though since reactions often occur in solution, defining a single combined 3D geometry for multiple reactants is non-trivial – we likely stick to per-molecule embeddings).

* **Alternative Reaction Datasets:** Aside from USPTO, there are other open reaction datasets we might leverage for specialized experts. The **USPTO-50k** dataset (50k reactions with classified reaction types) is often used for retrosynthesis and could be useful to train a **reaction-class specialist** expert. For example, one expert might focus on specific reaction classes (if the gating network first detects the reaction class, it could route to an expert that has seen many examples of that class). The 50k dataset’s class labels could supervise such an expert or gating mechanism. Another dataset is **USPTO-STEREO** (approximately 900k reactions with stereochemistry) – we could include those extra stereochemical reactions (beyond the 480k) to train experts to handle stereochemistry. This would increase data for cases where stereochemistry matters (e.g. chiral centers, E/Z double bonds). We must be cautious to maintain consistency in input format (we can extend our tokenization to include stereo markers or simply use RDKit to generate non-stereo SMILES for the main dataset to mix with stereo data). Finally, there are datasets for **selectivity** (which enantiomer or regioisomer is formed) and **yield prediction** (regression of yield given reactants and conditions). While these are somewhat orthogonal to pure product structure prediction, including them in a multi-expert framework could be fruitful. For instance, the model “ReactIR” (if available) predicts reaction yields from reagents and conditions – if we have any public data for that, a specialized expert could try to predict a yield or at least use yield as a weighted factor in choosing between possible products. However, given the scope, we will likely treat yields as ancillary information: e.g. as a re-ranking criterion (the condition expert might predict which product is higher yielding). In summary, we will primarily use **USPTO-480k** as the training backbone (to ensure comparability to prior SOTA models), but **augment it with**: (a) **Role and condition annotations** (from internal parsing or USPTO-Condition dataset) for a subset or all reactions; (b) **Pre-trained 3D features** from large molecular datasets; (c) **Additional reaction subsets** like USPTO-STEREO and USPTO-50k to provide stereo info and class labels. We will ensure all data is open-source or derived from open data (e.g. Lowe’s USPTO dataset), as requested. Proprietary sources like Reaxys or Pistachio will only be mentioned conceptually (they contain millions of reactions with conditions and yields, but we will not use them here due to license restrictions).

**Dataset-Expert Alignment:** Each expert in our MoE will benefit from different data:

* The **graph-based expert** can use the same USPTO data but represented as bond graphs (we have atom-mapped reactions, so constructing graph inputs is straightforward). It doesn’t need extra data beyond what Graph2SMILES used – though we could give it stereochem-rich data (USPTO-STEREO) to push its stereochemical accuracy.
* The **SMILES-based expert** will simply use USPTO reactions in tokenized SMILES form (mixed or separated). This expert essentially replicates the Molecular Transformer setting, so it serves as a strong baseline expert.
* The **3D expert** will use pre-trained knowledge (not a separate reaction dataset, but molecular datasets like QM9 for pre-training). During reaction training, it might still just see USPTO reactions, but with augmented features. If we find a source of reaction mechanism data (e.g. which bonds break/form – essentially the electron flow), we could further supervise the 3D expert (or a separate mechanistic expert) on learning those patterns. One example: the recent *NERF* model (Bi et al., 2021) predicted the electron redistribution matrix as an intermediate step. If any dataset of electron-pushing diagrams or mechanistic classes exists (some academic datasets label reactions by mechanism type), that could inform a specialized expert. Lacking a ready dataset, we might rely on unsupervised signals (the gating might cluster by reaction similarity which often correlates with mechanism).
* The **condition expert** will be trained on reactions that include conditions. We can create a combined dataset of USPTO-480k plus whatever portion has condition labels. For instance, there’s an open set of \~10k USPTO reactions with catalysts/solvents identified (from Schneider *et al.* 2021) – those could enhance the condition expert. Additionally, we might incorporate **reaction prediction with solvents** tasks: there was a Schneider group paper on predicting reagents and conditions (not just products). If available, the condition expert could be multi-tasked to also predict the catalyst or solvent (formulated as additional output tokens). This multi-task learning might improve its understanding of how conditions influence outcomes.

All told, our training pipeline will involve **harmonizing these datasets**: converting all reactions into a unified format that our model expects (likely the event-based token sequences with optional additional features). We will use scaffold split for evaluation to avoid overly easy memorization. The diverse data will be merged carefully: we will likely train in stages or with curricula (e.g. pre-train the graph expert on pure graph input first, pre-train the SMILES expert on a large corpus of unannotated reactions by self-supervised masking similar to Chemformer, etc., then fine-tune together).

## 3. **Heterogeneous MoE System Design**

We propose a **heterogeneous mixture-of-experts architecture** comprising multiple expert sub-models, each leveraging different input representations or inductive biases. A high-level design is as follows:

* **Input & Gating:** All experts ultimately aim to predict the reaction’s product SMILES from the reactant inputs. We will design a **gating network** that takes the reactants (and optionally reagents/conditions) as input and decides how to route the example to experts. One straightforward implementation is a small feed-forward neural network (or a lightweight Transformer) that reads an **input fingerprint** – for example, a Morgan fingerprint of the reactant set or a learned embedding from a shallow graph neural network – and outputs either a discrete choice (hard gate) or a probability distribution (soft gate) over experts. For efficiency, we lean towards **hard gating** (each reaction is handled by one expert network at a time), akin to Switch Transformer’s routing of tokens, but we will also experiment with gating to 2 or 3 experts with weighted averaging of their outputs (which might improve accuracy at the cost of more compute). The gating network will be trained alongside the experts, using the ground-truth product to backpropagate through whichever expert was chosen (in hard gating) or through a weighted combination (in soft gating). If hard gating is non-differentiable (due to a discrete argmax), we can use a softmax with a sharpened temperature or a policy gradient approach. However, an easier method is the **Switch-style straight-through**: select the top expert but backpropagate gradients as if the soft mixture were used (this is a known technique in MoE training). We will include a **load-balancing loss** on the gate to ensure it doesn’t always pick the same expert – e.g. an entropy term or the auxiliary load loss from Switch Transformer that encourages the assignment counts to be equal across experts.

* **Expert Architectures:** Each expert is a full model capable of predicting the product. We envision the following experts in our system (initially 3 or 4 experts):

  1. **Graph-based Expert:** This expert will mirror the Graph2SMILES architecture. It has a graph encoder that takes the set of reactant molecules (as one combined graph or individual graphs) and produces node embeddings with a D-MPNN, followed by a global attention pooling to get a single context representation. The decoder is a Transformer that attends over the graph encoder outputs to generate the product SMILES. We can re-use the open-source Graph2SMILES implementation (from our `references/Graph2SMILES` code) to build this expert, modifying it to fit within our MoE training loop. This expert excels at capturing structural relationships and should particularly help when the reaction outcome depends on connectivity (e.g., stereochemistry or distinguishing which reagents actually participate – Graph2SMILES naturally handles the permutation of reagents issue). We will incorporate the graph positional encoding (shortest-path distances) and stereochemical features as in the original model.

  2. **SMILES-based Expert:** A classic sequence model that treats the reaction as a sequence of tokens. We can base this on a Transformer encoder–decoder similar to the Molecular Transformer or Chemformer. In fact, we could initialize it with a pre-trained Chemformer model (since Chemformer’s public weights are available, pre-trained on millions of SMILES). This expert will input the reaction as a single concatenated string of reactants (with `>` separators or our role tokens). It will likely have learned positional encodings (sinusoidal or learned absolute positions). Because this expert uses no graph information, it provides a **baseline modality** – essentially what a strong text model would predict. We expect the SMILES expert to do well on common reactions (since sequence models have been very effective on USPTO) but perhaps struggle on cases requiring counting atoms or stereochemistry consistency (where the graph expert might do better). Using the Chemformer weights, fine-tuned on USPTO, could give it a head-start with chemical knowledge.

  3. **3D/Geometric Expert:** This expert will attempt to utilize 3D structural information of reactants or at least the 3D-infused features from pre-training. One design is a **graph-neural-network-to-transformer** model: e.g., a message-passing GNN that processes the 3D geometry of reactants (if available) or their 3DInfomax-augmented 2D graph, producing a set of node embeddings, and then a small transformer decoder for the product. Another design is to incorporate the 3D expert as an **adjunct to another expert** – for instance, augment the Graph-based expert with a 3D message-passing layer. However, to keep experts conceptually separate, we treat the 3D expert as its own module. It might share a lot of structure with the Graph expert, but with additional inputs: e.g., inter-atom distances or even predicted reaction center information. If we can obtain an external dataset of reaction mechanisms (like which bonds break/form), we could train the 3D expert to pay attention to those sites – perhaps by generating 3D conformers of reactants and highlighting the bonds that change (though identifying that from data is non-trivial without templates). As a simpler proxy, the 3D expert might focus on reactions where **3D matters**, such as those involving stereochemical inversion, bulky substituents (where steric hindrance decides the outcome), or conformational gating (certain intramolecular reactions happen only if groups can come into proximity). The gating network could learn to route such cases to the 3D expert. Technically, we will implement the 3D expert by taking the pre-trained GNN from 3DInfomax (Stärk et al.) and coupling it to a Transformer decoder. We will freeze or slowly fine-tune the GNN part so it retains the mutual-information-trained embeddings. Thus, the 3D expert’s encoder will output a representation that (hopefully) encodes things like likely reactive sites based on geometry or strain.

  4. **Condition-aware Expert:** This expert is built on the **ChemTransformer** internal model design. It will input sequences with the special role tokens (`[CATALYST]`, `[SOLVENT]`, etc.) and numerical tokens for conditions (e.g. temperature). Its architecture is a Transformer encoder–decoder with **learned positional encodings** that reset for each new molecule token. The idea is that this expert can explicitly use context like “in presence of Pd/C (catalyst) under 100 °C, predict product X”. This should excel in reactions where reagents or conditions strongly affect the outcome (e.g., whether a reaction produces product A vs B might depend on using an oxidizing agent or a specific catalyst). If we have data on yields, this expert could even output a yield estimate or at least incorporate the idea that the “major product” in the dataset could be influenced by yield (since sometimes dataset’s “major product” might depend on reaction conditions). We will train this expert on the portion of USPTO (or other data) where such annotations are available. In general, it will still output the product SMILES, but it has the advantage of richer input. A risk is that if we don’t actually have condition annotations for most training examples, the condition tokens will mostly be empty or generic. To mitigate this, we could *simulate* some condition variations: e.g., randomly designate one reagent as “solvent” in some training examples to teach the expert that solvents don’t change the product (just to prevent it from misinterpreting the format).

  5. **(Optional) Reaction-Pattern Expert:** In light of the sequential MoE paper, we may include an expert that specializes in certain reaction families or electron flow patterns. This could be implemented by training an expert on a subset of reactions (e.g., only pericyclic reactions, or only radical reactions) if we can classify the dataset. Alternatively, we could have an **expert classifier** as part of gating – for instance, first classify the reaction into N classes (like the USPTO reaction taxonomy of 10 categories, or perhaps a finer mechanism-based classification if available) and then route to the corresponding expert. However, unless we have a clear way to label patterns, this might overlap with what the gating network will learn to do on its own. We’ll keep this idea in reserve: our gating network could implicitly discover clusters corresponding to reaction types. If not, we could manually define a few broad classes (e.g. “acid-base neutralizations vs. organometallic couplings vs. rearrangements”) and assign one expert to each for specialization.

Each expert outputs a distribution over product tokens (they each have their own decoder and final linear layer to the vocabulary, though we will tie the vocabulary and tokenization scheme across experts for consistency). **During training**, we will either: (a) alternate training of experts, i.e. on each batch, use the gating network to pick an expert and only update that expert (and the gate); or (b) train all experts in parallel on all data, with the gating probabilities as weights (soft gating case). Method (a) is effectively like training an ensemble with a learned assignment – it’s simpler and avoids interfering gradients, but could suffer if the gating network is poor initially. We might start with **curriculum training**: first train each expert on the whole dataset for a few epochs (so they all have some basic ability to predict), then introduce the gating network and allow it to start specializing them. Alternatively, use the **sequential boosting** idea: train a first expert to convergence on USPTO, then freeze it; train a second expert on reactions the first got wrong; etc. This is easier to implement but yields a fixed ensemble rather than a flexible gating network at inference. However, the sequential approach’s strong results suggest we might incorporate a **boosting phase**: e.g., initialize experts via sequential training and then switch to a learned gate that refines the assignments. We will carefully monitor for **expert collapse** (all data going to one expert) – mitigation strategies include the gating loss (to spread assignments) and possibly a **routing dropout**, where with some probability a non-top expert is chosen during training to give it a chance to improve (this idea is akin to exploration in routing). We can also periodically freeze the gating network and force equal usage of experts for some epochs (ensuring each expert gets gradient updates).

* **Output Aggregation:** If using hard gating (one expert produces the prediction), then the output is simply that expert’s decoded SMILES. If using soft gating or multiple experts, we need to aggregate outputs. One way is to have experts output full ranked lists of products (beam search from each) and then merge these lists (like the 2024 paper did with a ranking step). We could rank by the model’s confidence or likelihood. Another approach is token-level mixing: e.g., at each decoding step, take a weighted sum of the experts’ probability distributions over next tokens (this would require the experts to decode in sync). This token-level mixture is complex given our experts are different architectures – so we prefer the *list aggregation*: have each expert independently propose outcomes (perhaps N=5 each), then combine and sort them. The gating network’s output probabilities could be used as part of the score (e.g., weight the log-likelihood from each expert by the gate probability, essentially a mixture model probability). In practice, we will likely generate the union of all experts’ top-5 and then take the top-5 of that union as the final predictions. This will be evaluated for top-N accuracy. For the single top-1 prediction used in training loss, if soft gating, it effectively averages the experts’ contributions; if hard gating, it’s whichever expert was chosen.

* **Gating Mechanism Details:** We will experiment with both **input feature-based gating** (a learned classifier on reactant features) and **sequential gating** as in Guo et al. 2024 (which was based on nearest neighbor in a learned embedding space). The input features for gating can be: concatenation of simple descriptors (like number of reactants, average molecular weight, presence of certain functional groups), or a learned embedding (we could feed the reactants into a small frozen ChemBERT or a message-passing network to get an embedding, then a linear layer to gate probabilities). Gating by *reaction class* could also be tried: e.g., first use a classifier to predict one of the 10 USPTO categories, then map that to an expert ID. However, not all differences we want to capture align with those coarse classes. Ideally, the gating network will learn to send, say, “radical polymerizations” to one expert and “Friedel–Crafts acylations” to another if that separation helps accuracy. We will monitor what the gate learns by analyzing the distribution of reaction types per expert. If we see imbalance or no clear specialization, we’ll adjust the gating architecture or losses.

**Pros and Cons – Hard vs Soft Gating:** Hard gating (one expert chosen) gives a clear division of labor and keeps inference cost low (only one model runs). It does risk that if the gate is slightly wrong, you miss the correct prediction even if another expert could have gotten it. Soft gating (multiple experts per input) provides a safety net and potentially allows combining complementary strengths (like an ensemble). But it increases compute roughly linearly with number of experts used, and blending model outputs can sometimes muddy the interpretability (one expert’s crisp prediction versus an average of many). We may compromise by using hard gating during training (to force specialization) but at inference, allow a few top experts to each generate a candidate and then pick the best – effectively a *post hoc* soft ensemble. This is similar to generating from all experts and ranking, as mentioned. The sequential MoE method effectively did this by always using all experts and ranking outputs. We might emulate that: **during inference, run a beam search on each expert (in parallel) and then rank all candidates**. The gating network can provide the ranking score or we simply use each model’s likelihood and perhaps add a small bonus to the expert that gating most strongly favored. This approach ensures we don’t miss an answer just because the gate had 0.4 probability on the correct expert but 0.6 on another – both will produce outputs, and if the “secondary” expert had the right product, it can still appear in top-5. The final user can see that as an option. This gives us the benefits of diversity (each expert’s different perspective) at inference time.

* **Training Strategy:** We have touched on this above, but to summarize: We will likely do a **two-phase training**. Phase 1: pre-train each expert independently on the full data or relevant subset. For example, train the graph expert like Graph2SMILES on USPTO for X epochs (possibly reaching \~90% accuracy on its own), train the SMILES expert like a Molecular Transformer, etc. This gives each expert a reasonable baseline capability. Phase 2: integrate the gating network and train the whole MoE: now each training example is routed and only the chosen expert is updated (plus the gate). We’ll use a small learning rate for already pre-trained components (to avoid destroying their knowledge) and a higher learning rate for the gate (which starts from scratch). We might freeze some experts initially while the gate learns to not “thrash” them, then unfreeze gradually. If any expert collapses (no assignments), we could add a small trick: occasionally force that expert to handle some batch to fine-tune it (even if gate would not choose it). Alternatively, incorporate a penalty in the gate loss for zero usage of an expert. We will also consider **multi-task training** if appropriate: for instance, the condition expert could have an auxiliary task of predicting reaction class or conditions. This might help it learn more robust features. All training will be done on the same train/val split of USPTO (likely an 80/10/10 split by scaffold as standard). We will use **early stopping on the validation set top-1 accuracy** to prevent overfitting, especially for smaller experts like the condition expert if it has less data. Each expert might have its own early-stop criterion, but since they are trained together in phase 2, we’ll monitor overall MoE performance. If one expert starts overfitting (e.g., its specialized data is small and it starts to degrade), we could regularize it or reduce its capacity.

* **Preventing Similar Experts:** One risk noted from our previous attempt with a homogeneous MoE decoder was that identical experts ended up learning very similar functions, yielding no gain. Here, by **heterogeneity** (different inputs and architectures) we inherently push them to specialize – the graph expert and SMILES expert won’t converge to the exact same internal representation because one sees adjacency information and the other sees a linear string. Additionally, we can enforce diversity by training on different data distributions if possible (e.g., fine-tune one expert on certain reaction classes more than others). The gating network will further encourage diversity by rewarding experts that are uniquely good for certain examples.

## 4. **Implementation Plan and Project Structure**

To manage this project, we will create a clear modular codebase (likely in PyTorch). The repository will be structured into the following components:

* **Data Processing Module:** Scripts to preprocess reaction datasets into the required formats. For USPTO, we’ll extend our current preprocessing (which produces tokenized CSV or NPZ files for ChemTransformer). We’ll add functions to (a) generate graph adjacency lists for reactants; (b) annotate tokens with roles and maybe group boundaries (for positional indices); (c) interface with RDKit for generating conformers or computing molecular descriptors for gating. This module will output a unified dataset object where each sample can provide different views: SMILES tokens, graph tensors, condition tokens, etc., so that each expert can get the data it needs. We will likely use PyTorch Geometric or DGL for graph representation to ease GNN implementation, and RDKit for any chemistry-specific featurization.

* **Model Definitions:** We’ll have a **directory for model components**, e.g. `models/`. Inside this, submodules for each expert type: `graph_expert.py`, `smiles_expert.py`, `condition_expert.py`, etc., each defining the architecture class for that expert (with its encoder & decoder). To maximize reuse, these might subclass a common base (for example, both the SMILES expert and condition expert are basically TransformerEnc-Dec models and can share an implementation, differing only in how input is embedded). We will incorporate code from our `references` where possible: e.g., the Graph2SMILES code for the graph expert’s encoder (DGAT or DGCNN layers), and the MolBart (Chemformer) code for the SMILES expert’s Transformer. We need to be mindful of code cleanliness: some reference repos might be complex, so we’ll likely extract the necessary model classes and not just copy entire training pipelines. The gating network will also be defined here (e.g. `gating_network.py`), which could be as simple as a single linear layer on top of a pooled reactant embedding. If we decide to use a more complex gating (like a tiny Transformer that encodes the reactants and then a classifier), that will be here too. Each expert model will expose a forward pass that produces either a distribution over next-token (for training with teacher forcing) or can generate outputs autoregressively (for inference). We will unify vocabulary across experts – e.g., a global `Vocab` object for SMILES tokens (including special tokens).

* **Mixture Model Integration:** A central class (e.g. `MoEModel`) will wrap the experts and gating. This class’s forward method will take an input and do the gate routing, then call the selected expert’s forward. If we use soft gating, it might call multiple experts and combine results. This class will handle the logic of training loss calculation: it will likely compute a cross-entropy loss for the output of whichever expert(s) were used. In case of soft gating, the loss could be a weighted sum of experts’ losses. We’ll include the gating auxiliary loss here as well (to encourage balanced usage). The MoEModel will have submodules for each expert (so that all parameters are under one roof for the optimizer). However, for clarity we might actually instantiate each expert separately and pass them around rather than a single monolithic model – PyTorch Lightning (if we use it) can still manage multiple parameter groups.

* **Training Pipeline:** We will provide training scripts (perhaps using PyTorch Lightning’s `Trainer` for simplicity, or a custom training loop if more flexibility needed). Likely a `train.py` that reads config files to instantiate the desired experts and gating, then trains. We will support multi-GPU training if available, since some experts (like the graph expert) and others (Transformer expert) could in principle be trained in parallel on different GPUs to speed things up. However, if we do hard gating, at each step only one expert is active, so multi-GPU parallelism would require a larger batch where different samples go to different experts – that’s a bit complex to load-balance. An alternative is to use data-parallel training but that usually duplicates the model on each GPU, which doesn’t directly help since one model contains all experts. Instead, we might adopt **model-parallel** or pipeline parallel ideas: assign different experts to different devices. Then the gating network directs data to the respective device. This could be efficient if many examples go to each expert per batch. In practice, load balancing might not be perfect, so one GPU could end up more loaded than another. As a simpler approach, we’ll likely start with single-GPU training (perhaps using a large 24GB GPU) for simplicity, and ensure the model fits by perhaps not activating all experts on every batch (hard gating naturally does this). We can also reduce batch size if memory is an issue because effectively one expert’s portion of the batch is the only one active.

* **Evaluation & Metrics:** We will have an evaluation script that computes top-1, top-3, top-5 accuracy on the test set, comparing the predicted product(s) to the ground truth. We will use canonical SMILES or the same atom-mapped format as input for fairness (most literature does canonical SMILES without mappings for evaluation). Beam search will be used for each expert to generate up to e.g. 5 candidates. If we do the combined output approach, we’ll generate from each expert and merge results. We also plan to measure **beam diversity** – e.g., the fraction of unique products in the top-5 or some diversity score – to verify if MoE produces more diverse guesses than a single model. Another evaluation is by reaction class: we can calculate accuracy per class to see if some expert particularly boosts certain classes. Additionally, we might test on **USPTO-50k** benchmark (the standard 50k set) to compare to literature models that often report on that (Graph2SMILES, etc.). Even though our focus is forward prediction, showing competitiveness on the 50k (which is easier, but allows class-wise breakdown) would be useful. If time permits, we can also evaluate on the **Pistachio 2017** set used by Schwaller et al. for external validation, or on **USPTO-MIT separated vs mixed** to ensure the model handles both input formats.

* **Testing & Ablation:** We will create a suite of tests/analysis scripts for ablation studies. For example: turn off one expert (or set the gating to ignore it) and see the drop in accuracy – this will quantify that expert’s contribution. Or enforce the gating to uniformly random and measure performance (should drop, showing the gate was useful). We will test extreme cases, e.g., what if we only feed a single type of reaction – does the gating consistently choose the expected expert? For instance, feed a bunch of simple addition reactions vs. a bunch of stereochem-heavy reactions, and see if gating decisions differ. This will help interpret the specialization. We will also use the validation set to tune hyperparameters: e.g. the weight of gating aux loss, the number of experts to activate (if soft gating), etc. Early stopping will likely monitor validation top-1 accuracy (overall MoE). We might also stop training an expert early if its performance saturates and continue training the others/gate – this could prevent overfitting by one component.

* **Frameworks and Scalability:** Using **PyTorch Lightning** is appealing for organizing this multi-component model, since it can handle multi-loss and multi-optimizer setups elegantly. We may give each expert its own optimizer or at least different LR (for pre-trained vs scratch). Lightning also makes multi-GPU easier. We will ensure the code is written with scalability in mind: e.g., using PyTorch’s DistributedDataParallel if needed. Memory-wise, the sum of parameters of all experts will be larger than a single model, but each by itself is not too huge (Graph2SMILES \~> 10M, MolTransformer \~> 12M, Chemformer \~45M base – together perhaps \~60–70M if all loaded, which is manageable on modern GPUs). The key is ensuring only necessary parts are active per batch to conserve activation memory. We will take advantage of PyTorch’s dynamic computation graph: for a given batch, we’ll index into the batch for each expert (like a mask of which samples routed to expert1, etc.), and run forward only on those. This conditional computation is a bit tricky to do in a **vectorized** way, but we can simply do it sequentially for each expert each batch – since different experts are disjoint subgraphs, this won’t backprop through inactive ones. It does mean we can’t use a single PyTorch `DataParallel` call easily, but Lightning might allow manual routing.

* **Repo Directory Layout:** Tentatively:

  ```
  MoE_experiments/hetero_moe
    data/ (raw and processed data files)
    preprocess/ (scripts to download/prepare data)
    models/
        gating_network.py
        graph_expert.py
        smiles_expert.py
        condition_expert.py
        ... 
        layers/ (maybe subdir for common layers like Transformer, MPNN etc.)
    training/
        train_moe.py
        train_expert_baseline.py (to pre-train single experts)
        lightning_modules.py (if using Lightning to define training loops)
    evaluation/
        evaluate_moe.py
        evaluate_baselines.py
        metrics.py (functions for top-n accuracy, etc.)
    utils/
        chem_utils.py (RDKit helper functions)
        data_utils.py (dataloaders, dataset classes)
        visualization.py (maybe for plotting results or attention maps)
    configs/
        default.yaml (global config with hyperparams and toggles)
        moe.yaml, graph_expert.yaml, ... (specific configurations)
    README.md (overview of how to run)
  ```

  This structure ensures each part of the pipeline is separated. For example, one could swap in a new expert by adding a file in `models/` and updating the MoE model class accordingly, without touching data processing or evaluation code.

* **High-Level Architecture Diagram:** The figure below conceptually illustrates our heterogeneous MoE setup. We have multiple expert models (Graph-based, SMILES-based, 3D-infused, Condition-infused), and a gating network that takes the reactants as input and determines which expert(s) will be invoked for prediction. During training, the gating and experts are updated jointly (with mechanisms to ensure balanced expert usage). During inference, either the gated expert’s output is taken as the prediction, or in a top-\$k\$ gating scenario, outputs from several experts are merged and ranked to produce the final predicted product.

*(Here, an architecture mind-map or flow diagram would be presented, showing the reactant input branching into different experts via a gating module, and then converging to output predictions. Each expert box would be labeled with its modality, e.g. “Graph2SMILES Expert,” “Molecular Transformer Expert,” etc., and the gating box might be labeled “routes input to expert.”)*

**Training & Evaluation Plan:** We will first verify that each expert alone performs near expected baselines (e.g., graph expert \~90%, SMILES expert \~90%). Then, integrate MoE and train on the training set (with scaffold split validation). We will compare the MoE’s validation accuracy to single models. Key metrics: **Top-1 accuracy** (primary), but also **Top-5 accuracy** – a successful MoE should especially improve top-\$N\$ if it generates diverse correct candidates that a single model might miss. We’ll also measure **invalid SMILES rate** (to ensure experts don’t produce gibberish; Graph2SMILES tends to always produce valid outputs due to atom-checking, whereas SMILES models might need to be constrained). We will run **ablation studies**: e.g., train a version with all experts but no gating (just an ensemble average) to see if learned gating beats a simple ensemble. Also try a single-expert variant with equivalent parameter count (for fairness, e.g. a larger Transformer matching the MoE in size) – this tests if MoE’s improvement is due to specialization versus just more parameters. We’ll document results like: MoE top-1 vs individual top-1, per-class accuracies, etc. We will also test on any external test sets (like the USPTO held-out test or time-split sets) to gauge generalization. Finally, we’ll validate the **usefulness of extra information**: Does the condition expert actually help on reactions where solvents/catalysts matter? One way to test: take a subset of reactions known to have important catalysts (e.g. cross-couplings) and see if MoE (with condition info) predicts better than a model without that info.

Throughout, we will cite comparisons to Graph2SMILES, Molecular Transformer, etc., to ensure our performance claims are grounded. For instance, if we achieve \~91% top-1 on USPTO-480k, that already ties the best literature (Chemformer \~91%). Any further gain, even 1-2%, would be notable. More importantly, if our MoE improves **top-5 accuracy** significantly (which is critical in practical use, since chemists often consider a few candidates), that’s a win – e.g., boosting top-5 from \~95% to \~97% would mean catching many of the previously missed solutions.

By the end of this implementation and evaluation, we expect to have a working **heterogeneous MoE reaction predictor** that leverages multi-modal chemical intelligence: a graph expert for structural reasoning, a language expert for broader coverage, a 3D expert for geometric considerations, and a conditions expert for context. This system will be benchmarked rigorously against prior single-expert models to quantify improvements, and its design will allow further extension (e.g., adding a retrosynthesis expert for backward prediction in future, or a bio-transformation expert if we include biochemical reactions). We will provide all code, trained model checkpoints, and a detailed analysis of the gating behavior and expert specializations as part of the final report.

**Sources:** The above findings and plans reference key literature and code: Graph2SMILES architecture and results, Molecular Transformer baseline, Chemformer pre-training gains, 3DInfomax datasets and improvements, the design of our internal ChemTransformer for condition encoding, the sequential MoE approach for rare patterns, and multi-modal MoE concepts from recent work. These inform the proposed architecture and justify each component of our heterogeneous MoE strategy.
